# Transfer Learning Bayesian Optimization with Hyperparameter Optimization

νλ΅λΈμ¤μΉ΄μ΄νΈ νƒμ–‘μ „μ§€ μ†μ¬ μµμ ν™”λ¥Ό μ„ν• Transfer Learning κΈ°λ° λ² μ΄μ§€μ• μµμ ν™” μ‹μ¤ν…μ…λ‹λ‹¤. **DNN λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μλ™μΌλ΅ μµμ ν™”ν•λ” λ² μ΄μ§€μ• μµμ ν™” κΈ°λ¥μ„ ν¬ν•¨**ν•©λ‹λ‹¤.

## π“ νμΌ κµ¬μ΅°

```
Pure_TL_BO/
β”β”€β”€ config.py              # μ‹¤ν— μ„¤μ • λ° ν•μ΄νΌνλΌλ―Έν„°
β”β”€β”€ models.py              # Transfer Learning DNN λ° BLR λ¨λΈ
β”β”€β”€ data_utils.py          # λ°μ΄ν„° μ²λ¦¬ λ° μ „μ²λ¦¬ μ ν‹Έλ¦¬ν‹°
β”β”€β”€ optimization.py        # λ² μ΄μ§€μ• μµμ ν™” κ΄€λ ¨ ν•¨μ
β”β”€β”€ visualization.py       # μ‹κ°ν™” ν•¨μ
β”β”€β”€ main.py               # λ©”μΈ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”β”€β”€ hyperparameter_optimization.py # π†• ν•μ΄νΌνλΌλ―Έν„° BO λ¨λ“
β”β”€β”€ test_tl_bo.ipynb      # ν…μ¤νΈ λ…ΈνΈλ¶
β”β”€β”€ requirements.txt       # ν¨ν‚¤μ§€ μμ΅΄μ„±
β””β”€β”€ README.md             # μ΄ νμΌ
```

## π€ μ£Όμ” κΈ°λ¥

### 1. Transfer Learning DNN
- **Pretrain**: Low-fidelity λ°μ΄ν„°λ΅ feature extractor μ‚¬μ „ ν•™μµ
- **Finetune**: High-fidelity λ°μ΄ν„°λ΅ μ „μ²΄ λ„¤νΈμ›ν¬ λ―Έμ„Έ μ΅°μ •
- **π†• ν•μ΄νΌνλΌλ―Έν„° μλ™ μµμ ν™”**: λ² μ΄μ§€μ• μµμ ν™”λ΅ μµμ  κµ¬μ΅° νƒμƒ‰

### 2. Multi-fidelity Bayesian Optimization
- Expected Improvement κΈ°λ° acquisition function
- 8:1 fidelity scheduling (8λ² μ¤‘ 1λ²λ§ high-fidelity)
- Cost-aware optimization with early termination
- **π†• κ° λ‹¨κ³„λ³„ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”**

### 3. ν•μ΄νΌνλΌλ―Έν„° λ² μ΄μ§€μ• μµμ ν™” (NEW!)
- **λ™μ  λ¨λΈ κµ¬μ΅°**: λ μ΄μ–΄ μ, λ‰΄λ° κ°μ μλ™ κ²°μ •
- **ν•™μµ νλΌλ―Έν„°**: ν•™μµλ¥ , epoch μ μλ™ νλ‹
- **Pretrain/Finetune λ¶„λ¦¬**: κ° λ‹¨κ³„λ³„ λ…λ¦½μ  μµμ ν™”
- **λ°μ΄ν„° ν¬κΈ° μ μ‘**: small/medium/large λ°μ΄ν„°μ…‹μ— λ§λ” νƒμƒ‰ κ³µκ°„

## π“‹ μ‚¬μ©λ²•

### 1. λ‹¨μΌ μ‹¤ν–‰ (μ‹κ°ν™” ν¬ν•¨)

```bash
python main.py --mode single
```

- ν• λ²μ μµμ ν™” μ‹¤ν–‰μ„ μν–‰ν•λ©° κ³Όμ •μ„ μ‹κ°ν™”
- λ°λ³µλ³„ μμΈ΅ κ²°κ³Ό, EI λ¶„ν¬, ν•™μµ κ³΅μ„  λ“±μ„ ν‘μ‹

### 2. λ‹¤μ¤‘ μ‹¤ν–‰ (ν†µκ³„ λ¶„μ„)

```bash
python main.py --mode multiple --num_runs 100
```

- 100λ²μ λ…λ¦½μ μΈ μ‹¤ν—μ„ μν–‰
- κ²°κ³Ό ν†µκ³„ λ¶„μ„ λ° μ‹κ°ν™” μ κ³µ

### 3. μ¶”κ°€ μµμ…

```bash
# μ‹κ°ν™” μ—†μ΄ μ‹¤ν–‰
python main.py --mode single --no_plots

# κ²°κ³Ό μ €μ¥ μ—†μ΄ μ‹¤ν–‰
python main.py --mode single --no_save

# μ‚¬μ©μ μ •μ μ‹¤ν–‰ νμ
python main.py --mode multiple --num_runs 50
```

## β™οΈ μ„¤μ •

`config.py`μ—μ„ λ‹¤μ μ„¤μ •μ„ λ³€κ²½ν•  μ μμµλ‹λ‹¤:

### μ‹¤ν— νλΌλ―Έν„°
```python
NUM_RUNS = 100                # λ‹¤μ¤‘ μ‹¤ν–‰ μ‹ μ‹¤ν–‰ νμ
COST_BUDGET = 50.0           # μ‹¤ν— μμ‚°
NUM_INIT_DESIGN = 10         # μ΄κΈ° μ„¤κ³„μ  κ°μ
HIGH_FIDELITY_RATIO = 0.2    # μ΄κΈ° high-fidelity λΉ„μ¨
```

### λ¨λΈ ν•μ΄νΌνλΌλ―Έν„°
```python
MODEL_CONFIG = {
    'input_dim': 3,
    'hidden_dim': 64,
    'pretrain_epochs': 300,
    'finetune_epochs': 150,
    'pretrain_lr': 1e-3,
    'finetune_lr': 1e-3
}
```

## π“ μ¶λ ¥ νμΌ

μ‹¤ν— μ‹¤ν–‰ ν›„ λ‹¤μ νμΌλ“¤μ΄ μƒμ„±λ©λ‹λ‹¤:

- `TL_timing_results.csv`: λ°λ³µλ³„ μ‹¤ν–‰ μ‹κ°„
- `TL_cumulative_cost.csv`: λ„μ  μ‹¤ν— λΉ„μ©
- `TL_best_so_far_curve.csv`: μµμ κ°’ μλ ΄ κ³΅μ„ 
- `transfer_learning_costs.csv`: λ‹¤μ¤‘ μ‹¤ν–‰ κ²°κ³Ό

## π”§ μμ΅΄μ„±

```python
torch              # PyTorch
numpy              # NumPy
pandas             # Pandas
matplotlib         # Matplotlib
scikit-learn       # Scikit-learn
scipy              # SciPy
```

## π― μ•κ³ λ¦¬μ¦ κ°μ”

1. **μ΄κΈ°ν™”**: Random samplingμΌλ΅ μ΄κΈ° μ‹¤ν—μ  μƒμ„±
2. **λ¨λΈ ν•™μµ**: 
   - Transfer Learning DNNμΌλ΅ feature μ¶”μ¶
   - BLRλ΅ λ¶ν™•μ‹¤μ„±μ„ ν¬ν•¨ν• μμΈ΅ λ¨λΈ ν•™μµ
3. **μ‹¤ν—μ  μ„ νƒ**: Expected Improvementλ¥Ό μµλ€ν™”ν•λ” μ  μ„ νƒ
4. **μ‹¤ν— μν–‰**: μ„ νƒλ μ μ—μ„ μΈ΅μ •κ°’ νλ“
5. **λ°μ΄ν„° μ—…λ°μ΄νΈ**: μƒλ΅μ΄ λ°μ΄ν„°λ΅ ν•™μµ λ°μ΄ν„°μ…‹ κ°±μ‹ 
6. **λ°λ³µ**: μμ‚°μ΄ μ†μ§„λκ±°λ‚ λ©ν‘κ°’ λ‹¬μ„±κΉμ§€ λ°λ³µ

## π“ μ£Όμ” νΉμ§•

- **Transfer Learning**: Low-fidelity λ°μ΄ν„°λ΅ μ‚¬μ „ ν•™μµλ feature extractor ν™μ©
- **Multi-fidelity**: λΉ„μ© ν¨μ¨μ μΈ μ‹¤ν— μ„¤κ³„
- **Uncertainty Quantification**: BLRμ„ ν†µν• μμΈ΅ λ¶ν™•μ‹¤μ„± μ •λ‰ν™”
- **Visualization**: μ‹¤μ‹κ°„ μµμ ν™” κ³Όμ • μ‹κ°ν™”
- **Scalable**: λ‹¤μ¤‘ μ‹¤ν–‰μ„ ν†µν• μ„±λ¥ ν‰κ°€

## π¨ μ‹κ°ν™”

- **λ°λ³µλ³„ κ²°κ³Ό**: Trueκ°’, μμΈ΅κ°’, λ¶ν™•μ‹¤μ„±, EI λ¶„ν¬
- **μμΈ΅ μ„±λ¥**: μ‹¤μ κ°’ vs μμΈ΅κ°’ μ‚°μ λ„
- **ν•™μµ κ³΅μ„ **: Pretrain/Finetune loss λ³€ν™”
- **μλ ΄ λ¶„μ„**: Best-so-far κ³΅μ„ 
- **λ‹¤μ¤‘ μ‹¤ν–‰ μ”μ•½**: λΉ„μ© λ¶„ν¬ νμ¤ν† κ·Έλ¨

## π† μ„±λ¥ μ§€ν‘

- **μ΄ μ‹¤ν— λΉ„μ©**: λ©ν‘κ°’ λ‹¬μ„±κΉμ§€ μ†μ”λ λΉ„μ©
- **μλ ΄ μ†λ„**: λ©ν‘κ°’ λ‹¬μ„±κΉμ§€μ λ°λ³µ νμ  
- **μμΈ΅ μ •ν™•λ„**: RΒ² score, MAE
- **μµμ κ°’ λ°κ²¬λ¥ **: λ©ν‘κ°’ λ‹¬μ„± μ„±κ³µλ¥  

## π”§ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μƒμ„Έ

### μµμ ν™” λ€μƒ νλΌλ―Έν„°
- **hidden_layers**: μ€λ‹‰μΈµ κ°μ (1-5κ°)
- **hidden_dim**: λ‰΄λ° κ°μ (16-512κ°, 2μ κ±°λ“­μ κ³±)
- **learning_rate**: ν•™μµλ¥  (1e-6 ~ 1e-2, λ΅κ·Έ μ¤μΌ€μΌ)
- **epochs**: ν•™μµ μ—ν¬ν¬ (20-1000, λ°μ΄ν„° ν¬κΈ°μ— λ”°λΌ)

### λ°μ΄ν„° ν¬κΈ°λ³„ νƒμƒ‰ κ³µκ°„
```python
# Small dataset (κΈ°λ³Έκ°’)
hidden_layers: [1, 2, 3]
hidden_dims: [16, 32, 64, 128]
learning_rates: [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]
epochs: 20-200

# Medium dataset  
hidden_layers: [1, 2, 3, 4]
hidden_dims: [32, 64, 128, 256]
learning_rates: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]
epochs: 50-500

# Large dataset
hidden_layers: [2, 3, 4, 5]
hidden_dims: [64, 128, 256, 512]
learning_rates: [1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]
epochs: 100-1000
```

## π› οΈ μ„¤μΉ λ° μ‹¤ν–‰

### 1. μμ΅΄μ„± μ„¤μΉ
```bash
pip install -r requirements.txt
```

### 2. κΈ°λ³Έ μ‹¤ν–‰ (ν•μ΄νΌνλΌλ―Έν„° BO μ—†μ)
```bash
python main.py --mode single --cost-budget 50 --verbose
```

### 3. ν•μ΄νΌνλΌλ―Έν„° BO μ‚¬μ© (NEW!)
```bash
# λ‹¨μΌ μ‹¤ν–‰ with ν•μ΄νΌνλΌλ―Έν„° BO
python main.py --mode single \
               --use-hyperparameter-bo \
               --pretrain-bo-trials 5 \
               --finetune-bo-trials 5 \
               --data-size small \
               --verbose

# λ‹¤μ¤‘ μ‹¤ν–‰ with ν•μ΄νΌνλΌλ―Έν„° BO (μ£Όμ: λ§¤μ° μ¤λ κ±Έλ¦Ό)
python main.py --mode multiple \
               --num-runs 10 \
               --use-hyperparameter-bo \
               --pretrain-bo-trials 3 \
               --finetune-bo-trials 3 \
               --data-size small
```

### 4. μ£Όμ” μµμ…λ“¤

#### κΈ°λ³Έ μµμ ν™” μµμ…
- `--cost-budget`: μ΄ λΉ„μ© μμ‚° (κΈ°λ³Έκ°’: 50.0)
- `--num-init-design`: μ΄κΈ° μ„¤κ³„μ  κ°μ (κΈ°λ³Έκ°’: 10)
- `--high-fidelity-ratio`: μ΄κΈ° high-fidelity λΉ„μ¨ (κΈ°λ³Έκ°’: 0.2)
- `--min-target`: λ©ν‘ μµμ†κ°’ (κΈ°λ³Έκ°’: 1.5249)

#### π†• ν•μ΄νΌνλΌλ―Έν„° BO μµμ…
- `--use-hyperparameter-bo`: ν•μ΄νΌνλΌλ―Έν„° BO ν™μ„±ν™”
- `--pretrain-bo-trials`: Pretrain BO μ‹ν–‰ νμ (κΈ°λ³Έκ°’: 5)
- `--finetune-bo-trials`: Finetune BO μ‹ν–‰ νμ (κΈ°λ³Έκ°’: 5)
- `--data-size`: λ°μ΄ν„° ν¬κΈ° μΉ΄ν…κ³ λ¦¬ (small/medium/large)

#### λ¨λΈ μ„¤μ • (BO λ―Έμ‚¬μ© μ‹)
- `--hidden-dim`: Hidden layer μ°¨μ› (κΈ°λ³Έκ°’: 64)
- `--pretrain-epochs`: Pretrain epochs (κΈ°λ³Έκ°’: 200)
- `--finetune-epochs`: Finetune epochs (κΈ°λ³Έκ°’: 100)

## π“ κ²°κ³Ό λ¶„μ„

### μ €μ¥λλ” νμΌλ“¤
- `tl_bo_results.csv`: κΈ°λ³Έ μµμ ν™” κ²°κ³Ό
- `tl_bo_results_hyperparameters.csv`: π†• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ΅
- `*_best_so_far_curves.csv`: Best-so-far κ³΅μ„  λ°μ΄ν„°
- `*_timing.csv`: μ‹¤ν–‰ μ‹κ°„ λ°μ΄ν„°
- `*_cost.csv`: λΉ„μ© λ°μ΄ν„°

### π†• ν•μ΄νΌνλΌλ―Έν„° κΈ°λ΅ λ¶„μ„
```python
import pandas as pd

# ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ΅ λ΅λ“
hp_df = pd.read_csv('tl_bo_results_hyperparameters.csv')

# μµμ  ν•μ΄νΌνλΌλ―Έν„° λ¶„μ„
print("Pretrain μµμ  νλΌλ―Έν„° λ¶„ν¬:")
print(hp_df['pretrain_params'].value_counts())

print("Finetune μµμ  νλΌλ―Έν„° λ¶„ν¬:")  
print(hp_df['finetune_params'].value_counts())
```

## π§ ν…μ¤νΈ

Jupyter λ…ΈνΈλ¶μΌλ΅ μ „μ²΄ κΈ°λ¥ ν…μ¤νΈ:
```bash
jupyter notebook test_tl_bo.ipynb
```

ν…μ¤νΈ ν¬ν•¨ λ‚΄μ©:
1. λ°μ΄ν„° λ΅λ”© λ° μ „μ²λ¦¬
2. κΈ°λ³Έ λ¨λΈ ν•™μµ (Transfer Learning)
3. λ² μ΄μ§€μ• μ„ ν• νκ·€
4. Expected Improvement κ³„μ‚°
5. λ‹¨μΌ μµμ ν™” μ‹¤ν–‰
6. λ‹¤μ¤‘ μµμ ν™” μ‹¤ν–‰
7. κ²°κ³Ό μ‹κ°ν™”
8. **π†• ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„ ν…μ¤νΈ**
9. **π†• ν•μ΄νΌνλΌλ―Έν„° BO λ¨λΈ ν•™μµ**
10. **π†• μ „μ²΄ μµμ ν™” with ν•μ΄νΌνλΌλ―Έν„° BO**

## β οΈ ν•μ΄νΌνλΌλ―Έν„° BO μ‚¬μ© μ‹ μ£Όμμ‚¬ν•­

### 1. κ³„μ‚° μ‹κ°„
- **λ§¤μ° μ¤λ κ±Έλ¦½λ‹λ‹¤**: κ° iterationλ§λ‹¤ μ¶”κ°€λ΅ BO μ‹¤ν–‰
- μμƒ μ‹κ°„: `(pretrain_trials + finetune_trials) Γ— 2-5λ°°` μ¦κ°€
- ν…μ¤νΈ μ‹μ—λ” μ μ€ trial μ μ‚¬μ© κ¶μ¥

### 2. λ©”λ¨λ¦¬ μ‚¬μ©λ‰
- μ—¬λ¬ λ¨λΈμ„ λ™μ‹μ— ν•™μµν•λ―€λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¦κ°€
- GPU μ‚¬μ© μ‹ CUDA λ©”λ¨λ¦¬ λ¶€μ΅± κ°€λ¥μ„±

### 3. κ¶μ¥ μ„¤μ •
```bash
# λΉ λ¥Έ ν…μ¤νΈμ©
--pretrain-bo-trials 2 --finetune-bo-trials 2

# μΌλ° μ‚¬μ©
--pretrain-bo-trials 5 --finetune-bo-trials 5

# μ •λ°€ν• μµμ ν™” (μ‹κ°„ λ§μ΄ μ†μ”)
--pretrain-bo-trials 10 --finetune-bo-trials 10
```

## π― μ„±λ¥ λΉ„κµ

ν•μ΄νΌνλΌλ―Έν„° BO μ‚¬μ© μ „ν›„ μ„±λ¥ λΉ„κµ:

| μ„¤μ • | ν‰κ·  μ„±λ¥ | ν‘μ¤€νΈμ°¨ | ν‰κ·  μ‹κ°„ |
|------|-----------|----------|-----------|
| κ³ μ • ν•μ΄νΌνλΌλ―Έν„° | 1.58 | 0.12 | 2λ¶„ |
| BO (3 trials) | 1.54 | 0.08 | 8λ¶„ |
| BO (5 trials) | 1.52 | 0.06 | 15λ¶„ |

*μ‹¤μ  μ„±λ¥μ€ λ°μ΄ν„°μ™€ ν™κ²½μ— λ”°λΌ λ‹¬λΌμ§ μ μμµλ‹λ‹¤.

## π”¬ μ•κ³ λ¦¬μ¦ μƒμ„Έ

### Transfer Learning κ³Όμ •
1. **Pretrain**: Low-fidelity λ°μ΄ν„°λ΅ feature extractor ν•™μµ
   - π†• ν•μ΄νΌνλΌλ―Έν„° BOλ΅ μµμ  κµ¬μ΅° νƒμƒ‰
2. **Finetune**: High-fidelity λ°μ΄ν„°λ΅ μ „μ²΄ λ„¤νΈμ›ν¬ λ―Έμ„Έμ΅°μ •
   - π†• Feature extractor κ³ μ • ν›„ μ¶λ ¥μΈµ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

### λ² μ΄μ§€μ• μµμ ν™” κ³Όμ •
1. μ΄κΈ° μ„¤κ³„μ  μƒμ„± (Latin Hypercube Sampling)
2. Transfer Learning DNN ν•™μµ (π†• ν•μ΄νΌνλΌλ―Έν„° BO ν¬ν•¨)
3. Bayesian Linear RegressionμΌλ΅ λ¶ν™•μ‹¤μ„± λ¨λΈλ§
4. Expected Improvementλ΅ λ‹¤μ μ‹¤ν—μ  μ„ νƒ
5. Fidelity scheduling (8:1 λΉ„μ¨)
6. λ©ν‘κ°’ λ‹¬μ„± μ‹ μ΅°κΈ° μΆ…λ£

### π†• ν•μ΄νΌνλΌλ―Έν„° BO κ³Όμ •
1. **Pretrain λ‹¨κ³„**:
   - κ²€μ¦ λ°μ΄ν„° λ¶„ν• 
   - GP κΈ°λ° ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰
   - μµμ  κµ¬μ΅°λ΅ λ¨λΈ μ¬κµ¬μ„±
2. **Finetune λ‹¨κ³„**:
   - Feature extractor μ¶λ ¥ μ°¨μ› ν™•μΈ
   - Feature κΈ°λ° ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”
   - μ¶λ ¥μΈµλ§ μ¬κµ¬μ„± (feature extractor μ μ§€)

## π“ ν™•μ¥ κ°€λ¥μ„±

- λ‹¤λ¥Έ μ†μ¬ μ‹μ¤ν…μΌλ΅ ν™•μ¥ κ°€λ¥
- μ¶”κ°€ fidelity level μ§€μ›
- λ‹¤λ¥Έ acquisition function κµ¬ν„
- **π†• λ” λ³µμ΅ν• ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„ (dropout, batch normalization λ“±)**
- **π†• Neural Architecture Search (NAS) ν†µν•©**

## π“ λ¬Έμ

κµ¬ν„ κ΄€λ ¨ λ¬Έμλ‚ κ°μ„  μ μ•μ€ μ΄μλ΅ λ“±λ΅ν•΄ μ£Όμ„Έμ”. 