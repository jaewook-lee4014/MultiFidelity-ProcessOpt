import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Optional, Tuple
from sklearn.model_selection import train_test_split


class TransferLearningDNN:
    """
    Transfer Learningì„ ìœ„í•œ Deep Neural Network í´ë˜ìŠ¤
    - Pretrain: low-fidelity ë°ì´í„°ë¡œ feature extractor í•™ìŠµ
    - Finetune: high-fidelity ë°ì´í„°ë¡œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¯¸ì„¸ì¡°ì •
    - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë² ì´ì§€ì•ˆ ìµœì í™” ì§€ì›
    """
    
    def __init__(self, input_dim, hidden_dim=64, device='cpu', use_hyperparameter_bo=False):
        self.input_dim = input_dim
        self.device = device
        self.hidden_dim = hidden_dim
        self.use_hyperparameter_bo = use_hyperparameter_bo
        self.pretrain_losses = []
        self.finetune_losses = []
        
        # BO ê´€ë ¨ ë³€ìˆ˜
        self.pretrain_best_params = None
        self.finetune_best_params = None
        self.pretrain_bo_history = []
        self.finetune_bo_history = []

        # ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡° (BO ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ)
        if not use_hyperparameter_bo:
            self._build_default_model(hidden_dim)
    
    def _build_default_model(self, hidden_dim):
        """ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        # feature extractor (hidden layers)
        self.feature_net = nn.Sequential(
            nn.Linear(self.input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        ).to(self.device)
        
        # ì¶œë ¥ì¸µ
        self.out_layer = nn.Linear(hidden_dim, 1, bias=False).to(self.device)
        
        # ì „ì²´ ëª¨ë¸
        self.model = nn.Sequential(self.feature_net, self.out_layer)
        
        # float32ë¡œ ì„¤ì •
        self.feature_net = self.feature_net.float()
        self.out_layer = self.out_layer.float()
        self.model = self.model.float()
    
    def _build_dynamic_model(self, params: Dict):
        """ë™ì  ëª¨ë¸ êµ¬ì¡° ìƒì„± (BO ê²°ê³¼ ê¸°ë°˜)"""
        layers = []
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        layers.append(nn.Linear(self.input_dim, params['hidden_dim']))
        layers.append(nn.ReLU())
        
        # ì¤‘ê°„ ë ˆì´ì–´ë“¤
        for _ in range(params['hidden_layers'] - 1):
            layers.append(nn.Linear(params['hidden_dim'], params['hidden_dim']))
            layers.append(nn.ReLU())
        
        self.feature_net = nn.Sequential(*layers).to(self.device)
        self.out_layer = nn.Linear(params['hidden_dim'], 1, bias=False).to(self.device)
        self.model = nn.Sequential(self.feature_net, self.out_layer)
        
        # float32ë¡œ ì„¤ì •
        self.feature_net = self.feature_net.float()
        self.out_layer = self.out_layer.float()
        self.model = self.model.float()
    
    def _split_validation_data(self, X: np.ndarray, y: np.ndarray, val_ratio: float = 0.2) -> Tuple:
        """ê²€ì¦ ë°ì´í„° ë¶„í• """
        if len(X) < 3:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë¶„í• í•˜ì§€ ì•ŠìŒ
            return X, y, X, y
        
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=val_ratio, random_state=42
        )
        return X_train, y_train, X_val, y_val

    def pretrain(self, X_low, y_low, epochs=50, lr=1e-3, verbose=False, 
                 bo_trials=None, data_size='small'):
        """
        Low-fidelity ë°ì´í„°ë¡œ pretrain
        
        Args:
            X_low: low-fidelity ì…ë ¥ ë°ì´í„°
            y_low: low-fidelity ì¶œë ¥ ë°ì´í„°
            epochs: ê¸°ë³¸ epoch ìˆ˜ (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
            lr: ê¸°ë³¸ í•™ìŠµë¥  (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
            verbose: ìƒì„¸ ì¶œë ¥
            bo_trials: BO ì‹œí–‰ íšŸìˆ˜ (Noneì´ë©´ BO ì‚¬ìš© ì•ˆí•¨)
            data_size: ë°ì´í„° í¬ê¸° ('small', 'medium', 'large')
        """
        self.pretrain_losses = []
        X_low = np.asarray(X_low, dtype=np.float32)
        y_low = np.asarray(y_low, dtype=np.float32).flatten()
        
        if verbose:
            print(f"Pretrain with {len(X_low)} low-fidelity samples")
        
        if self.use_hyperparameter_bo and bo_trials is not None and bo_trials > 0:
            # ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°
            if verbose:
                print(f"ğŸ” Optimizing pretrain hyperparameters with {bo_trials} trials...")
            
            # ê²€ì¦ ë°ì´í„° ë¶„í• 
            X_train, y_train, X_val, y_val = self._split_validation_data(X_low, y_low)
            
            # BO ì‹¤í–‰
            from hyperparameter_optimization import optimize_dnn_hyperparameters
            best_params, best_performance, history = optimize_dnn_hyperparameters(
                X_train, y_train, X_val, y_val, 
                input_dim=self.input_dim,
                n_trials=bo_trials,
                data_size=data_size,
                device=self.device,
                verbose=verbose
            )
            
            self.pretrain_best_params = best_params
            self.pretrain_bo_history = history
            
            if verbose:
                print(f"âœ… Best pretrain params: {best_params}")
                print(f"âœ… Best validation loss: {best_performance:.4f}")
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ êµ¬ì„±
            self._build_dynamic_model(best_params)
            epochs = best_params['epochs']
            lr = best_params['learning_rate']
        else:
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
            if not hasattr(self, 'model'):
                self._build_default_model(self.hidden_dim)
        
        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
        X_tensor = torch.tensor(X_low, dtype=torch.float32).to(self.device)
        y_tensor = torch.tensor(y_low, dtype=torch.float32).view(-1, 1).to(self.device)
        optimizer = optim.Adam(list(self.feature_net.parameters()) + list(self.out_layer.parameters()), lr=lr)
        loss_fn = nn.MSELoss()

        self.model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            features = self.feature_net(X_tensor)
            pred = self.out_layer(features)
            loss = loss_fn(pred, y_tensor)
            loss.backward()
            optimizer.step()
            self.pretrain_losses.append(loss.item())
            
            if verbose and (epoch+1) % max(1, epochs//10) == 0:
                print(f'[Pretrain] Epoch {epoch+1}/{epochs}: Loss {loss.item():.4f}')

    def finetune(self, X_high, y_high, epochs=50, lr=1e-4, verbose=False,
                 bo_trials=None, data_size='small'):
        """
        High-fidelity ë°ì´í„°ë¡œ finetune
        
        Args:
            X_high: high-fidelity ì…ë ¥ ë°ì´í„°
            y_high: high-fidelity ì¶œë ¥ ë°ì´í„°
            epochs: ê¸°ë³¸ epoch ìˆ˜ (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
            lr: ê¸°ë³¸ í•™ìŠµë¥  (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
            verbose: ìƒì„¸ ì¶œë ¥
            bo_trials: BO ì‹œí–‰ íšŸìˆ˜ (Noneì´ë©´ BO ì‚¬ìš© ì•ˆí•¨)
            data_size: ë°ì´í„° í¬ê¸° ('small', 'medium', 'large')
        """
        self.finetune_losses = []
        X_high = np.asarray(X_high, dtype=np.float32)
        y_high = np.asarray(y_high, dtype=np.float32).flatten()
        
        if verbose:
            print(f"Finetune with {len(X_high)} high-fidelity samples")
        
        if self.use_hyperparameter_bo and bo_trials is not None and bo_trials > 0:
            # ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°
            if verbose:
                print(f"ğŸ” Optimizing finetune hyperparameters with {bo_trials} trials...")
            
            # ê²€ì¦ ë°ì´í„° ë¶„í• 
            X_train, y_train, X_val, y_val = self._split_validation_data(X_high, y_high)
            
            # í˜„ì¬ feature extractorì˜ ì¶œë ¥ ì°¨ì› í™•ì¸
            with torch.no_grad():
                sample_input = torch.tensor(X_train[:1], dtype=torch.float32).to(self.device)
                feature_dim = self.feature_net(sample_input).shape[1]
            
            # BO ì‹¤í–‰ (featureë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©)
            X_train_features = self.extract_features(X_train)
            X_val_features = self.extract_features(X_val)
            
            from hyperparameter_optimization import optimize_dnn_hyperparameters
            best_params, best_performance, history = optimize_dnn_hyperparameters(
                X_train_features, y_train, X_val_features, y_val,
                input_dim=feature_dim,
                n_trials=bo_trials,
                data_size=data_size,
                device=self.device,
                verbose=verbose
            )
            
            self.finetune_best_params = best_params
            self.finetune_bo_history = history
            
            if verbose:
                print(f"âœ… Best finetune params: {best_params}")
                print(f"âœ… Best validation loss: {best_performance:.4f}")
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¶œë ¥ì¸µë§Œ ì¬êµ¬ì„± (feature extractorëŠ” ìœ ì§€)
            self.out_layer = nn.Linear(feature_dim, 1, bias=False).to(self.device).float()
            self.model = nn.Sequential(self.feature_net, self.out_layer)
            
            epochs = best_params['epochs']
            lr = best_params['learning_rate']
        
        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
        X_tensor = torch.tensor(X_high, dtype=torch.float32).to(self.device)
        y_tensor = torch.tensor(y_high, dtype=torch.float32).view(-1, 1).to(self.device)
        optimizer = optim.Adam(list(self.feature_net.parameters()) + list(self.out_layer.parameters()), lr=lr)
        loss_fn = nn.MSELoss()
        
        self.model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            features = self.feature_net(X_tensor)
            pred = self.out_layer(features)
            loss = loss_fn(pred, y_tensor)
            loss.backward()
            optimizer.step()
            self.finetune_losses.append(loss.item())
            
            if verbose and (epoch+1) % max(1, epochs//10) == 0:
                print(f'[Finetune] Epoch {epoch+1}/{epochs}: Loss {loss.item():.4f}')

    def predict(self, X):
        """ì˜ˆì¸¡"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)
            pred = self.model(X_tensor)
            return pred.cpu().numpy().flatten()

    def extract_features(self, X):
        """Feature ì¶”ì¶œ"""
        self.feature_net.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)
            features = self.feature_net(X_tensor)
            return features.cpu().numpy()
    
    def get_hyperparameter_summary(self) -> Dict:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ìš”ì•½"""
        summary = {
            'use_hyperparameter_bo': self.use_hyperparameter_bo,
            'pretrain_best_params': self.pretrain_best_params,
            'finetune_best_params': self.finetune_best_params,
            'pretrain_trials': len(self.pretrain_bo_history),
            'finetune_trials': len(self.finetune_bo_history)
        }
        return summary


class BayesianLinearRegression:
    """
    ë² ì´ì§€ì•ˆ ì„ í˜• íšŒê·€ ëª¨ë¸
    ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨í•œ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, alpha=1.0, beta=25.0):
        """
        Args:
            alpha: ê°€ì¤‘ì¹˜ì˜ ì •ë°€ë„ (precision) íŒŒë¼ë¯¸í„°
            beta: ë…¸ì´ì¦ˆì˜ ì •ë°€ë„ íŒŒë¼ë¯¸í„°
        """
        self.alpha = alpha
        self.beta = beta
        self.mean = None
        self.cov = None
        self.fitted = False
    
    def fit(self, X, y):
        """
        ë² ì´ì§€ì•ˆ ì„ í˜• íšŒê·€ í•™ìŠµ
        
        Args:
            X: ì…ë ¥ íŠ¹ì„± (N x D)
            y: íƒ€ê²Ÿ ê°’ (N,)
        """
        X = np.asarray(X, dtype=np.float32)
        y = np.asarray(y, dtype=np.float32).flatten()
        
        # í¸í–¥ í•­ ì¶”ê°€
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        
        # ì‚¬ì „ ë¶„í¬: w ~ N(0, Î±^(-1)I)
        S0_inv = self.alpha * np.eye(X_with_bias.shape[1])
        
        # ì‚¬í›„ ë¶„í¬ ê³„ì‚°
        S_N_inv = S0_inv + self.beta * X_with_bias.T @ X_with_bias
        self.cov = np.linalg.inv(S_N_inv)
        self.mean = self.beta * self.cov @ X_with_bias.T @ y
        
        self.fitted = True
    
    def predict(self, x):
        """
        ë‹¨ì¼ ì ì— ëŒ€í•œ ì˜ˆì¸¡
        
        Args:
            x: ì…ë ¥ íŠ¹ì„± ë²¡í„° (D,)
            
        Returns:
            í‰ê· , ë¶„ì‚°
        """
        if not self.fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        x = np.asarray(x, dtype=np.float32).flatten()
        x_with_bias = np.concatenate([[1], x])
        
        # ì˜ˆì¸¡ í‰ê· 
        mu = x_with_bias @ self.mean
        
        # ì˜ˆì¸¡ ë¶„ì‚°
        var = (1/self.beta) + x_with_bias @ self.cov @ x_with_bias
        
        return mu, var
    
    def predict_batch(self, X):
        """
        ë°°ì¹˜ ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ íŠ¹ì„± í–‰ë ¬ (N x D)
            
        Returns:
            í‰ê· ë“¤, ë¶„ì‚°ë“¤
        """
        X = np.asarray(X, dtype=np.float32)
        means = []
        variances = []
        
        for x in X:
            mu, var = self.predict(x)
            means.append(mu)
            variances.append(var)
        
        return np.array(means), np.array(variances) 