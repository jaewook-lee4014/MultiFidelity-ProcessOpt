import numpy as np
import itertools
import time
from scipy.stats import norm
from typing import List, Tuple, Dict, Optional
from models import TransferLearningDNN, BayesianLinearRegression


def expected_improvement(mu: np.ndarray, sigma: np.ndarray, y_best: float, xi: float = 0.01) -> np.ndarray:
    """
    Expected Improvement ê³„ì‚°
    
    Args:
        mu: ì˜ˆì¸¡ í‰ê· 
        sigma: ì˜ˆì¸¡ í‘œì¤€í¸ì°¨
        y_best: í˜„ì¬ê¹Œì§€ì˜ ìµœì ê°’
        xi: exploration parameter
        
    Returns:
        Expected Improvement ê°’
    """
    sigma = np.maximum(sigma, 1e-8)
    z = (y_best - mu - xi) / sigma
    ei = (y_best - mu - xi) * norm.cdf(z) + sigma * norm.pdf(z)
    return ei


def penalized_expected_improvement(mu: np.ndarray, sigma: np.ndarray, y_best: float, 
                                  xi: float = 0.01, penalty_scale: float = 1.0, 
                                  penalty_func=None) -> np.ndarray:
    """
    Penalized Expected Improvement ê³„ì‚°
    """
    ei = expected_improvement(mu, sigma, y_best, xi)
    if penalty_func is not None:
        penalty = penalty_func(mu, sigma)
        ei = ei - penalty_scale * penalty
    return ei


def train_model(X_low: np.ndarray, y_low: np.ndarray, X_high: np.ndarray, y_high: np.ndarray,
                input_dim: int = 3, hidden_dim: int = 64, device: str = 'cpu',
                pretrain_epochs: int = 200, finetune_epochs: int = 100,
                pretrain_lr: float = 1e-3, finetune_lr: float = 1e-4,
                use_hyperparameter_bo: bool = False, pretrain_bo_trials: int = 0,
                finetune_bo_trials: int = 0, data_size: str = 'small',
                verbose: bool = False) -> TransferLearningDNN:
    """
    Transfer Learning DNN ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° BO ì§€ì›)
    
    Args:
        X_low: low-fidelity ì…ë ¥ ë°ì´í„°
        y_low: low-fidelity ì¶œë ¥ ë°ì´í„°
        X_high: high-fidelity ì…ë ¥ ë°ì´í„°
        y_high: high-fidelity ì¶œë ¥ ë°ì´í„°
        input_dim: ì…ë ¥ ì°¨ì›
        hidden_dim: ê¸°ë³¸ hidden ì°¨ì› (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
        device: ë””ë°”ì´ìŠ¤
        pretrain_epochs: ê¸°ë³¸ pretrain epochs (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
        finetune_epochs: ê¸°ë³¸ finetune epochs (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
        pretrain_lr: ê¸°ë³¸ pretrain í•™ìŠµë¥  (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
        finetune_lr: ê¸°ë³¸ finetune í•™ìŠµë¥  (BO ì‚¬ìš© ì‹œ ë¬´ì‹œë¨)
        use_hyperparameter_bo: í•˜ì´í¼íŒŒë¼ë¯¸í„° BO ì‚¬ìš© ì—¬ë¶€
        pretrain_bo_trials: pretrain BO ì‹œí–‰ íšŸìˆ˜
        finetune_bo_trials: finetune BO ì‹œí–‰ íšŸìˆ˜
        data_size: ë°ì´í„° í¬ê¸° ('small', 'medium', 'large')
        verbose: ìƒì„¸ ì¶œë ¥
        
    Returns:
        í•™ìŠµëœ TransferLearningDNN ëª¨ë¸
    """
    model = TransferLearningDNN(
        input_dim=input_dim, 
        hidden_dim=hidden_dim, 
        device=device,
        use_hyperparameter_bo=use_hyperparameter_bo
    )
    
    # Pretrain with low-fidelity data
    if len(X_low) > 0:
        if use_hyperparameter_bo and pretrain_bo_trials > 0:
            model.pretrain(
                X_low, y_low, 
                epochs=pretrain_epochs, lr=pretrain_lr, verbose=verbose,
                bo_trials=pretrain_bo_trials, data_size=data_size
            )
        else:
            model.pretrain(
                X_low, y_low, 
                epochs=pretrain_epochs, lr=pretrain_lr, verbose=verbose
            )
    
    # Finetune with high-fidelity data
    if len(X_high) > 0:
        if use_hyperparameter_bo and finetune_bo_trials > 0:
            model.finetune(
                X_high, y_high, 
                epochs=finetune_epochs, lr=finetune_lr, verbose=verbose,
                bo_trials=finetune_bo_trials, data_size=data_size
            )
        else:
            model.finetune(
                X_high, y_high, 
                epochs=finetune_epochs, lr=finetune_lr, verbose=verbose
            )
    
    return model


def fit_blr(model: TransferLearningDNN, X_low: np.ndarray, X_high: np.ndarray, 
            y_low: np.ndarray, y_high: np.ndarray, alpha: float = 1.0, beta: float = 25.0):
    """
    Bayesian Linear Regression í•™ìŠµ
    """
    # ì „ì²´ ë°ì´í„° ê²°í•©
    X_all = np.vstack([X_low, X_high]) if len(X_high) > 0 else X_low
    y_all = np.concatenate([y_low, y_high]) if len(y_high) > 0 else y_low
    
    # Feature ì¶”ì¶œ
    features_all = model.extract_features(X_all)
    
    # BLR í•™ìŠµ
    blr = BayesianLinearRegression(alpha=alpha, beta=beta)
    blr.fit(features_all, y_all)
    
    return blr, X_all, y_all


def recommend_next(model: TransferLearningDNN, blr: BayesianLinearRegression, 
                   param_ranges: List[range], X_low: np.ndarray, X_high: np.ndarray,
                   y_low: np.ndarray, y_high: np.ndarray, s: float) -> Tuple:
    """
    ë‹¤ìŒ ì‹¤í—˜ì  ì¶”ì²œ (Expected Improvement ìµœëŒ€í™”)
    """
    # ì „ì²´ ì¡°í•© ìƒì„±
    all_combinations = list(itertools.product(*param_ranges))
    X_grid = np.array(all_combinations, dtype=np.float32)
    
    # í˜„ì¬ê¹Œì§€ì˜ ìµœì ê°’ (high-fidelityë§Œ ê³ ë ¤)
    if len(y_high) > 0:
        y_best = np.min(y_high)
    else:
        y_best = np.inf
    
    # ì „ì²´ ì¡°í•©ì— ëŒ€í•œ ì˜ˆì¸¡
    features_grid = model.extract_features(X_grid)
    y_pred, y_std = [], []
    
    for phi in features_grid:
        mu, var = blr.predict(phi)
        y_pred.append(mu)
        y_std.append(np.sqrt(var))
    
    y_pred = np.array(y_pred)
    y_std = np.array(y_std)
    
    # Expected Improvement ê³„ì‚°
    ei = expected_improvement(y_pred, y_std, y_best)
    
    # ì´ë¯¸ ì¸¡ì •ëœ ì ë“¤ì€ ì œì™¸
    measured_points = set()
    for x in X_low:
        measured_points.add(tuple(x.astype(int)))
    for x in X_high:
        measured_points.add(tuple(x.astype(int)))
    
    # ì¸¡ì •ë˜ì§€ ì•Šì€ ì ë“¤ë§Œ ê³ ë ¤
    valid_indices = []
    for i, combo in enumerate(X_grid):
        if tuple(combo.astype(int)) not in measured_points:
            valid_indices.append(i)
    
    if not valid_indices:
        # ëª¨ë“  ì ì´ ì¸¡ì •ëœ ê²½ìš° (ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
        best_idx = np.argmax(ei)
    else:
        # ìœ íš¨í•œ ì ë“¤ ì¤‘ì—ì„œ ìµœëŒ€ EI ì„ íƒ
        valid_ei = ei[valid_indices]
        best_valid_idx = np.argmax(valid_ei)
        best_idx = valid_indices[best_valid_idx]
    
    next_x_label = list(X_grid[best_idx].astype(int))
    
    return next_x_label, y_pred, y_std, ei, best_idx, X_grid


def single_optimization_run(param_space: Dict, label_maps: Dict, lookup: Dict,
                           cost_budget: float = 50.0, num_init_design: int = 10,
                           high_fidelity_ratio: float = 0.2, min_target: float = 1.5249,
                           random_state: int = 42, verbose: bool = True,
                           model_config: Dict = None, 
                           use_hyperparameter_bo: bool = False,
                           pretrain_bo_trials: int = 0, finetune_bo_trials: int = 0,
                           data_size: str = 'small') -> Dict:
    """
    ë‹¨ì¼ ìµœì í™” ì‹¤í–‰ (í•˜ì´í¼íŒŒë¼ë¯¸í„° BO ì§€ì›)
    
    Args:
        param_space: íŒŒë¼ë¯¸í„° ê³µê°„
        label_maps: ë¼ë²¨ ë§¤í•‘
        lookup: lookup table
        cost_budget: ë¹„ìš© ì˜ˆì‚°
        num_init_design: ì´ˆê¸° ì„¤ê³„ì  ê°œìˆ˜
        high_fidelity_ratio: high-fidelity ë¹„ìœ¨
        min_target: ëª©í‘œ ìµœì†Ÿê°’
        random_state: ëœë¤ ì‹œë“œ
        verbose: ìƒì„¸ ì¶œë ¥
        model_config: ëª¨ë¸ ì„¤ì •
        use_hyperparameter_bo: í•˜ì´í¼íŒŒë¼ë¯¸í„° BO ì‚¬ìš© ì—¬ë¶€
        pretrain_bo_trials: pretrain BO ì‹œí–‰ íšŸìˆ˜
        finetune_bo_trials: finetune BO ì‹œí–‰ íšŸìˆ˜
        data_size: ë°ì´í„° í¬ê¸°
        
    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ë¹„ìš©, best_so_far ê³¡ì„ , ì‹œê°„ ë“±)
    """
    from data_utils import (
        sample_param_space, assign_fidelities, prepare_initial_data,
        measure_from_label, append_measurement_to_data
    )
    
    if model_config is None:
        model_config = {
            'input_dim': 3,
            'hidden_dim': 64,
            'pretrain_epochs': 200,
            'finetune_epochs': 100,
            'device': 'cpu'
        }
    
    # íŒŒë¼ë¯¸í„° ë²”ìœ„
    param_ranges = [
        range(1, len(param_space['organic']) + 1),
        range(1, len(param_space['cation']) + 1),
        range(1, len(param_space['anion']) + 1),
    ]
    
    # ì´ˆê¸° ì„¤ê³„
    init_samples = sample_param_space(param_space, num_init_design, random_state=random_state)
    init_fids = assign_fidelities(num_init_design, high_fidelity_ratio, random_state=random_state)
    
    # ì´ˆê¸° ë°ì´í„° ì¤€ë¹„
    X_low, y_low, X_high, y_high = prepare_initial_data(init_samples, init_fids, label_maps, lookup)
    
    # ì´ˆê¸° ë¹„ìš© ê³„ì‚°
    total_cost = sum(init_fids)
    
    # ì¶”ì  ë³€ìˆ˜ë“¤
    best_so_far = np.inf  # high-fidelity ì¸¡ì •ê°’ë§Œìœ¼ë¡œ ì¶”ì 
    best_so_far_curve = []
    timing_data = []
    cost_data = []
    iter_ = 0
    hyperparameter_history = []  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
    
    # ì´ˆê¸° best_so_far ì„¤ì •
    if len(y_high) > 0:
        best_so_far = np.min(y_high)
    
    if verbose:
        print(f"Initial cost: {total_cost:.2f}, Initial best_so_far: {best_so_far}")
        if use_hyperparameter_bo:
            print(f"ğŸ”§ Using hyperparameter BO: pretrain={pretrain_bo_trials}, finetune={finetune_bo_trials}")
    
    # ë©”ì¸ ìµœì í™” ë£¨í”„
    while total_cost < cost_budget:
        iter_ += 1
        iter_start = time.time()
        
        if verbose:
            print(f"\n==== Iteration {iter_} ====")
        
        # Fidelity ìŠ¤ì¼€ì¤„ë§: 8ë²ˆ ì¤‘ 1ë²ˆë§Œ high-fidelity
        s = 1.0 if (iter_ % 8 == 0) else 0.1
        
        # ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° BO í¬í•¨)
        model = train_model(
            X_low, y_low, X_high, y_high, 
            input_dim=model_config['input_dim'],
            hidden_dim=model_config['hidden_dim'],
            device=model_config['device'],
            pretrain_epochs=model_config['pretrain_epochs'],
            finetune_epochs=model_config['finetune_epochs'],
            use_hyperparameter_bo=use_hyperparameter_bo,
            pretrain_bo_trials=pretrain_bo_trials,
            finetune_bo_trials=finetune_bo_trials,
            data_size=data_size,
            verbose=verbose
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
        if use_hyperparameter_bo:
            hp_summary = model.get_hyperparameter_summary()
            hyperparameter_history.append({
                'iteration': iter_,
                'pretrain_params': hp_summary['pretrain_best_params'],
                'finetune_params': hp_summary['finetune_best_params']
            })
            
            if verbose and hp_summary['pretrain_best_params']:
                print(f"ğŸ”§ Pretrain params: {hp_summary['pretrain_best_params']}")
            if verbose and hp_summary['finetune_best_params']:
                print(f"ğŸ”§ Finetune params: {hp_summary['finetune_best_params']}")
        
        # BLR í•™ìŠµ
        blr, X_all, y_all = fit_blr(model, X_low, X_high, y_low, y_high)
        
        # ë‹¤ìŒ ì‹¤í—˜ì  ì¶”ì²œ
        next_x_label, y_pred, y_std, ei, best_idx, X_grid = recommend_next(
            model, blr, param_ranges, X_low, X_high, y_low, y_high, s
        )
        
        # ì¸¡ì •
        measurement = measure_from_label(next_x_label, s, label_maps, lookup)
        
        if verbose:
            print(f"Recommended: {next_x_label} (fidelity: {s})")
            print(f"Measurement: {measurement:.4f}")
            print(f"Max EI: {ei[best_idx]:.6f}")
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸
        X_low, y_low, X_high, y_high = append_measurement_to_data(
            X_low, y_low, X_high, y_high, next_x_label, s, label_maps, lookup
        )
        
        # ë¹„ìš© ë° ì‹œê°„ ì—…ë°ì´íŠ¸
        iter_end = time.time()
        time_taken = iter_end - iter_start
        total_cost += s
        
        # best_so_far ì—…ë°ì´íŠ¸ (high-fidelityë§Œ)
        if s == 1.0:
            if measurement < best_so_far:
                best_so_far = measurement
        
        # ê¸°ë¡
        timing_data.append([0, iter_, time_taken])
        cost_data.append([0, iter_, total_cost])
        best_so_far_curve.append([0, iter_, s, best_so_far])
        
        if verbose:
            print(f"Cumulative cost: {total_cost:.2f}, best_so_far: {best_so_far:.4f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
        if s == 1.0 and np.isclose(measurement, min_target, atol=1e-6):
            if verbose:
                print('Found the minimum target value!')
            break
    
    return {
        'total_cost': total_cost,
        'best_so_far': best_so_far,
        'iterations': iter_,
        'best_so_far_curve': best_so_far_curve,
        'timing_data': timing_data,
        'cost_data': cost_data,
        'final_X_low': X_low,
        'final_y_low': y_low,
        'final_X_high': X_high,
        'final_y_high': y_high,
        'hyperparameter_history': hyperparameter_history,
        'use_hyperparameter_bo': use_hyperparameter_bo
    }


def multiple_optimization_runs(param_space: Dict, label_maps: Dict, lookup: Dict,
                              num_runs: int = 100, cost_budget: float = 50.0,
                              num_init_design: int = 10, high_fidelity_ratio: float = 0.2,
                              min_target: float = 1.5249, model_config: Dict = None,
                              save_results: bool = True, results_filename: str = 'tl_bo_results.csv',
                              use_hyperparameter_bo: bool = False, pretrain_bo_trials: int = 0,
                              finetune_bo_trials: int = 0, data_size: str = 'small') -> List[Dict]:
    """
    ë‹¤ì¤‘ ìµœì í™” ì‹¤í–‰ (í•˜ì´í¼íŒŒë¼ë¯¸í„° BO ì§€ì›)
    """
    import pandas as pd
    
    all_results = []
    all_costs = []
    
    print(f"Starting {num_runs} optimization runs...")
    if use_hyperparameter_bo:
        print(f"ğŸ”§ Using hyperparameter BO: pretrain={pretrain_bo_trials}, finetune={finetune_bo_trials}")
    
    for run in range(num_runs):
        print(f"\n===== Run {run+1}/{num_runs} =====")
        
        result = single_optimization_run(
            param_space=param_space,
            label_maps=label_maps,
            lookup=lookup,
            cost_budget=cost_budget,
            num_init_design=num_init_design,
            high_fidelity_ratio=high_fidelity_ratio,
            min_target=min_target,
            random_state=run,  # ê° ëŸ°ë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œ
            verbose=False,  # ë‹¤ì¤‘ ì‹¤í–‰ ì‹œ ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”
            model_config=model_config,
            use_hyperparameter_bo=use_hyperparameter_bo,
            pretrain_bo_trials=pretrain_bo_trials,
            finetune_bo_trials=finetune_bo_trials,
            data_size=data_size
        )
        
        all_results.append(result)
        all_costs.append(result['total_cost'])
        
        if result['best_so_far'] <= min_target:
            print(f"Run {run+1}: Found target! Cost: {result['total_cost']:.2f}")
        else:
            print(f"Run {run+1}: Completed. Cost: {result['total_cost']:.2f}, Best: {result['best_so_far']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    if save_results:
        results_df = pd.DataFrame({
            'run': range(1, num_runs + 1),
            'total_cost': all_costs,
            'best_so_far': [r['best_so_far'] for r in all_results],
            'iterations': [r['iterations'] for r in all_results],
            'use_hyperparameter_bo': [r['use_hyperparameter_bo'] for r in all_results]
        })
        results_df.to_csv(results_filename, index=False)
        print(f"\nResults saved to {results_filename}")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡ ì €ì¥ (BO ì‚¬ìš© ì‹œ)
        if use_hyperparameter_bo:
            all_hp_records = []
            for run_idx, result in enumerate(all_results):
                for hp_record in result['hyperparameter_history']:
                    hp_record['run'] = run_idx + 1
                    all_hp_records.append(hp_record)
            
            if all_hp_records:
                hp_df = pd.DataFrame(all_hp_records)
                hp_filename = results_filename.replace('.csv', '_hyperparameters.csv')
                hp_df.to_csv(hp_filename, index=False)
                print(f"Hyperparameter history saved to {hp_filename}")
    
    # ìš”ì•½ í†µê³„
    success_rate = sum(1 for r in all_results if r['best_so_far'] <= min_target) / num_runs
    avg_cost = np.mean(all_costs)
    std_cost = np.std(all_costs)
    
    print(f"\n=== Summary Statistics ===")
    print(f"Success rate: {success_rate:.2%}")
    print(f"Average cost: {avg_cost:.2f} Â± {std_cost:.2f}")
    print(f"Min cost: {np.min(all_costs):.2f}")
    print(f"Max cost: {np.max(all_costs):.2f}")
    
    return all_results 