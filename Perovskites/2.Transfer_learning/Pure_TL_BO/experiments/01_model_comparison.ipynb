{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Experiment\n",
    "실험 날짜: 2024-11-12\n",
    "목적: 다양한 모델의 BO 성능 비교\n",
    "\n",
    "## 실험 설정\n",
    "- Budget: 50\n",
    "- Target: 1.34 eV (perovskite bandgap)\n",
    "- Seeds: 5개 (재현성 검증)\n",
    "- Models: Transfer Learning DNN, GP, RF, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 기본 설정\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# 실험 관리 도구\n",
    "from experiment_runner import ExperimentRunner, model_registry\n",
    "from data_utils import load_lookup_table, create_param_space\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Experiment started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "lookup = load_lookup_table('../../0.Data/lookup_table.pkl')\n",
    "param_space = create_param_space(lookup)\n",
    "\n",
    "print(f\"Parameter space dimensions:\")\n",
    "for key, values in param_space.items():\n",
    "    print(f\"  {key}: {len(values)} options\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_space.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 실험 설정\n",
    "BASE_CONFIG = {\n",
    "    'budget': 50,\n",
    "    'target_value': 1.34,\n",
    "    'num_initial': 5,\n",
    "    'high_fidelity_ratio': 0.125,  # 8:1 ratio\n",
    "    'pretrain_epochs': 100,\n",
    "    'finetune_epochs': 50,\n",
    "    'device': 'cpu'  # or 'cuda' if available\n",
    "}\n",
    "\n",
    "# 테스트할 모델들\n",
    "MODELS = {\n",
    "    'TL-DNN': {\n",
    "        'class': 'tl_dnn',\n",
    "        'params': {\n",
    "            'hidden_dim': 128,\n",
    "            'n_layers': 3,\n",
    "            'dropout_rate': 0.1\n",
    "        }\n",
    "    },\n",
    "    'TL-DNN-Small': {\n",
    "        'class': 'tl_dnn',\n",
    "        'params': {\n",
    "            'hidden_dim': 64,\n",
    "            'n_layers': 2,\n",
    "            'dropout_rate': 0.1\n",
    "        }\n",
    "    },\n",
    "    'TL-DNN-Large': {\n",
    "        'class': 'tl_dnn',\n",
    "        'params': {\n",
    "            'hidden_dim': 256,\n",
    "            'n_layers': 4,\n",
    "            'dropout_rate': 0.2\n",
    "        }\n",
    "    },\n",
    "    'TL-DNN-HyperOpt': {\n",
    "        'class': 'tl_dnn_hyperopt',\n",
    "        'params': {\n",
    "            'pretrain_bo_trials': 5,\n",
    "            'finetune_bo_trials': 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Models to test: {list(MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 모델 인스턴스 생성\n",
    "model_instances = {}\n",
    "\n",
    "for name, config in MODELS.items():\n",
    "    try:\n",
    "        model = model_registry.create(config['class'], **config['params'])\n",
    "        model_instances[name] = model\n",
    "        print(f\"✓ Created {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create {name}: {e}\")\n",
    "        # Fallback: 직접 모델 생성\n",
    "        from models import TransferLearningDNN\n",
    "        model = TransferLearningDNN(**config['params'])\n",
    "        model_instances[name] = model\n",
    "        print(f\"  → Using fallback for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 실험 실행\n",
    "runner = ExperimentRunner(base_dir='experiment_results')\n",
    "N_SEEDS = 5  # 각 모델당 5번 실행\n",
    "\n",
    "print(f\"\\nRunning {len(model_instances)} models × {N_SEEDS} seeds = {len(model_instances) * N_SEEDS} total experiments\")\n",
    "print(\"This may take a while...\\n\")\n",
    "\n",
    "# 비교 실행\n",
    "results_df = runner.run_comparison(\n",
    "    models=model_instances,\n",
    "    base_config=BASE_CONFIG,\n",
    "    n_seeds=N_SEEDS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 결과 분석\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 모델별 통계\n",
    "stats = results_df.groupby('model').agg({\n",
    "    'best_value': ['mean', 'std', 'min', 'max'],\n",
    "    'total_cost': ['mean', 'std', 'min', 'max'],\n",
    "    'n_iterations': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Statistics:\")\n",
    "print(stats)\n",
    "\n",
    "# 목표값과의 차이\n",
    "target = BASE_CONFIG['target_value']\n",
    "results_df['gap_from_target'] = abs(results_df['best_value'] - target)\n",
    "\n",
    "gap_stats = results_df.groupby('model')['gap_from_target'].agg(['mean', 'std', 'min'])\n",
    "print(\"\\nGap from Target (1.34 eV):\")\n",
    "print(gap_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 시각화: Box plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Best value distribution\n",
    "results_df.boxplot(column='best_value', by='model', ax=axes[0, 0])\n",
    "axes[0, 0].axhline(y=target, color='r', linestyle='--', label='Target')\n",
    "axes[0, 0].set_title('Best Value Distribution')\n",
    "axes[0, 0].set_ylabel('Bandgap (eV)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Total cost distribution\n",
    "results_df.boxplot(column='total_cost', by='model', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Total Cost Distribution')\n",
    "axes[0, 1].set_ylabel('Cost')\n",
    "\n",
    "# 3. Gap from target\n",
    "results_df.boxplot(column='gap_from_target', by='model', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Gap from Target')\n",
    "axes[1, 0].set_ylabel('|Value - 1.34| (eV)')\n",
    "\n",
    "# 4. Iterations\n",
    "results_df.boxplot(column='n_iterations', by='model', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Number of Iterations')\n",
    "axes[1, 1].set_ylabel('Iterations')\n",
    "\n",
    "plt.suptitle('Model Comparison Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 시각화: Performance vs Efficiency\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# 각 모델의 평균 성능 계산\n",
    "model_means = results_df.groupby('model').agg({\n",
    "    'gap_from_target': 'mean',\n",
    "    'total_cost': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Scatter plot\n",
    "for model in model_means['model'].unique():\n",
    "    model_data = results_df[results_df['model'] == model]\n",
    "    ax.scatter(model_data['total_cost'], \n",
    "              model_data['gap_from_target'],\n",
    "              label=model, alpha=0.6, s=100)\n",
    "\n",
    "# 평균점 표시\n",
    "ax.scatter(model_means['total_cost'], \n",
    "          model_means['gap_from_target'],\n",
    "          s=200, edgecolors='black', linewidth=2, \n",
    "          marker='*', c='red', label='Mean')\n",
    "\n",
    "# Annotations\n",
    "for _, row in model_means.iterrows():\n",
    "    ax.annotate(row['model'], \n",
    "               (row['total_cost'], row['gap_from_target']),\n",
    "               xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Total Cost', fontsize=12)\n",
    "ax.set_ylabel('Gap from Target (eV)', fontsize=12)\n",
    "ax.set_title('Performance vs Efficiency Trade-off', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Statistical significance test\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = results_df['model'].unique()\n",
    "print(\"\\nPairwise t-tests for gap_from_target:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, model1 in enumerate(models):\n",
    "    for model2 in models[i+1:]:\n",
    "        data1 = results_df[results_df['model'] == model1]['gap_from_target']\n",
    "        data2 = results_df[results_df['model'] == model2]['gap_from_target']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "        \n",
    "        print(f\"{model1} vs {model2}:\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(f\"  → Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → No significant difference\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Best performing configuration\n",
    "best_run = results_df.loc[results_df['gap_from_target'].idxmin()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST PERFORMING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {best_run['model']}\")\n",
    "print(f\"Seed: {best_run['seed']}\")\n",
    "print(f\"Best value: {best_run['best_value']:.6f} eV\")\n",
    "print(f\"Gap from target: {best_run['gap_from_target']:.6f} eV\")\n",
    "print(f\"Total cost: {best_run['total_cost']:.2f}\")\n",
    "print(f\"Iterations: {best_run['n_iterations']}\")\n",
    "print(f\"Experiment ID: {best_run['experiment_id']}\")\n",
    "\n",
    "# Load detailed results\n",
    "detailed_results = runner.load_results(best_run['experiment_id'])\n",
    "if 'best_params' in detailed_results:\n",
    "    print(f\"\\nBest parameters found:\")\n",
    "    for key, value in detailed_results['best_params'].items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save summary report\n",
    "report = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'base_config': BASE_CONFIG,\n",
    "    'models_tested': list(MODELS.keys()),\n",
    "    'n_seeds': N_SEEDS,\n",
    "    'summary_statistics': stats.to_dict(),\n",
    "    'best_configuration': best_run.to_dict(),\n",
    "    'results_file': f\"experiment_results/results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "}\n",
    "\n",
    "import json\n",
    "report_path = f\"experiment_results/report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_path}\")\n",
    "print(f\"Results saved to: {results_df.to_csv('experiment_results/comparison_results.csv', index=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings\n",
    "1. Model performance ranking:\n",
    "   - \n",
    "\n",
    "2. Efficiency analysis:\n",
    "   - \n",
    "\n",
    "3. Robustness (variance across seeds):\n",
    "   - \n",
    "\n",
    "### Recommendations\n",
    "- \n",
    "\n",
    "### Next Steps\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}