#!/usr/bin/env python3
"""
Transfer Learning Bayesian Optimization with Hyperparameter Optimization
Î©îÏù∏ Ïã§Ìñâ Ïä§ÌÅ¨Î¶ΩÌä∏ - ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Î≤†Ïù¥ÏßÄÏïà ÏµúÏ†ÅÌôî ÏßÄÏõê
"""

import time
import pandas as pd
import numpy as np
import torch
from typing import Dict, List, Tuple
import argparse
import sys
import os
from pathlib import Path

# Î°úÏª¨ Î™®Îìà import
from config import *
from data_utils import (
    load_lookup_table, create_label_maps, sample_param_space, 
    assign_fidelities, prepare_initial_data, create_all_combinations_data,
    create_param_space
)
from optimization import single_optimization_run, multiple_optimization_runs
from visualization import (
    plot_iteration_results, plot_prediction_scatter, 
    plot_multiple_runs_summary, plot_learning_curves,
    plot_optimization_results
)

# ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨Î•º Python pathÏóê Ï∂îÍ∞Ä
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))


def setup_device():
    """ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï (GPU/CPU)"""
    if DEVICE_CONFIG['use_gpu'] and torch.cuda.is_available():
        device = 'cuda'
        print("CUDA ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.")
    elif DEVICE_CONFIG['use_gpu'] and torch.backends.mps.is_available():
        device = 'mps'
        print("MPS ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.")
    else:
        device = 'cpu'
        print("CPUÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
    return device


def parse_arguments():
    """Î™ÖÎ†πÌñâ Ïù∏Ïàò ÌååÏã±"""
    parser = argparse.ArgumentParser(
        description='Transfer Learning Bayesian Optimization with Hyperparameter BO'
    )
    
    # Í∏∞Î≥∏ Ïã§Ìñâ ÏòµÏÖò
    parser.add_argument('--mode', choices=['single', 'multiple'], default='single',
                       help='Ïã§Ìñâ Î™®Îìú: single run ÎòêÎäî multiple runs')
    parser.add_argument('--num-runs', type=int, default=10,
                       help='Multiple runs Î™®ÎìúÏóêÏÑú Ïã§Ìñâ ÌöüÏàò')
    parser.add_argument('--verbose', action='store_true',
                       help='ÏÉÅÏÑ∏ Ï∂úÎ†• ÌôúÏÑ±Ìôî')
    
    # ÏµúÏ†ÅÌôî ÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument('--cost-budget', type=float, default=50.0,
                       help='Ï¥ù ÎπÑÏö© ÏòàÏÇ∞')
    parser.add_argument('--num-init-design', type=int, default=10,
                       help='Ï¥àÍ∏∞ ÏÑ§Í≥ÑÏ†ê Í∞úÏàò')
    parser.add_argument('--high-fidelity-ratio', type=float, default=0.2,
                       help='Ï¥àÍ∏∞ ÏÑ§Í≥ÑÏóêÏÑú high-fidelity ÎπÑÏú®')
    parser.add_argument('--min-target', type=float, default=1.5249,
                       help='Î™©Ìëú ÏµúÏÜüÍ∞í (Ï°∞Í∏∞ Ï¢ÖÎ£å Ï°∞Í±¥)')
    
    # Î™®Îç∏ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument('--hidden-dim', type=int, default=64,
                       help='Í∏∞Î≥∏ hidden layer Ï∞®Ïõê')
    parser.add_argument('--pretrain-epochs', type=int, default=200,
                       help='Í∏∞Î≥∏ pretrain epochs')
    parser.add_argument('--finetune-epochs', type=int, default=100,
                       help='Í∏∞Î≥∏ finetune epochs')
    parser.add_argument('--device', default='cpu',
                       help='PyTorch ÎîîÎ∞îÏù¥Ïä§ (cpu/cuda)')
    
    # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ BO ÏòµÏÖò
    parser.add_argument('--use-hyperparameter-bo', action='store_true',
                       help='ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Î≤†Ïù¥ÏßÄÏïà ÏµúÏ†ÅÌôî ÏÇ¨Ïö©')
    parser.add_argument('--pretrain-bo-trials', type=int, default=5,
                       help='Pretrain ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ BO ÏãúÌñâ ÌöüÏàò')
    parser.add_argument('--finetune-bo-trials', type=int, default=5,
                       help='Finetune ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ BO ÏãúÌñâ ÌöüÏàò')
    parser.add_argument('--data-size', choices=['small', 'medium', 'large'], default='small',
                       help='Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ (ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ Í≥µÍ∞Ñ Í≤∞Ï†ï)')
    
    # Í≤∞Í≥º Ï†ÄÏû•
    parser.add_argument('--save-results', action='store_true', default=True,
                       help='Í≤∞Í≥ºÎ•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•')
    parser.add_argument('--results-filename', default='tl_bo_results.csv',
                       help='Í≤∞Í≥º ÌååÏùºÎ™Ö')
    parser.add_argument('--plot-results', action='store_true',
                       help='Í≤∞Í≥º ÏãúÍ∞ÅÌôî')
    
    return parser.parse_args()


def setup_model_config(args):
    """Î™®Îç∏ ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±"""
    return {
        'input_dim': 3,  # organic, cation, anion
        'hidden_dim': args.hidden_dim,
        'pretrain_epochs': args.pretrain_epochs,
        'finetune_epochs': args.finetune_epochs,
        'device': args.device
    }


def print_configuration(args, param_space, model_config):
    """ÏÑ§Ï†ï Ï†ïÎ≥¥ Ï∂úÎ†•"""
    print("=" * 60)
    print("Transfer Learning Bayesian Optimization Configuration")
    print("=" * 60)
    print(f"Mode: {args.mode}")
    if args.mode == 'multiple':
        print(f"Number of runs: {args.num_runs}")
    
    print(f"\nOptimization Parameters:")
    print(f"  Cost budget: {args.cost_budget}")
    print(f"  Initial design points: {args.num_init_design}")
    print(f"  High-fidelity ratio: {args.high_fidelity_ratio}")
    print(f"  Target minimum: {args.min_target}")
    
    print(f"\nParameter Space:")
    print(f"  Organic materials: {len(param_space['organic'])}")
    print(f"  Cations: {len(param_space['cation'])}")
    print(f"  Anions: {len(param_space['anion'])}")
    print(f"  Total combinations: {len(param_space['organic']) * len(param_space['cation']) * len(param_space['anion'])}")
    
    print(f"\nModel Configuration:")
    print(f"  Input dimension: {model_config['input_dim']}")
    print(f"  Hidden dimension: {model_config['hidden_dim']}")
    print(f"  Pretrain epochs: {model_config['pretrain_epochs']}")
    print(f"  Finetune epochs: {model_config['finetune_epochs']}")
    print(f"  Device: {model_config['device']}")
    
    # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ BO ÏÑ§Ï†ï
    if args.use_hyperparameter_bo:
        print(f"\nüîß Hyperparameter Bayesian Optimization:")
        print(f"  Enabled: Yes")
        print(f"  Data size category: {args.data_size}")
        print(f"  Pretrain BO trials: {args.pretrain_bo_trials}")
        print(f"  Finetune BO trials: {args.finetune_bo_trials}")
        print(f"  ‚ö†Ô∏è  Note: This will significantly increase computation time!")
    else:
        print(f"\nüîß Hyperparameter Bayesian Optimization:")
        print(f"  Enabled: No (using fixed hyperparameters)")
    
    print("=" * 60)


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    args = parse_arguments()
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    print("Loading data...")
    lookup = load_lookup_table(DATA_PATHS['lookup_table'])
    label_maps = create_label_maps(PARAM_SPACE)
    param_space = create_param_space(lookup)
    
    # Î™®Îç∏ ÏÑ§Ï†ï
    model_config = setup_model_config(args)
    
    # ÏÑ§Ï†ï Ï∂úÎ†•
    print_configuration(args, param_space, model_config)
    
    # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ BO ÏÇ¨Ïö© Ïãú Í≤ΩÍ≥†
    if args.use_hyperparameter_bo:
        total_bo_trials = args.pretrain_bo_trials + args.finetune_bo_trials
        estimated_time_per_iter = total_bo_trials * 2  # ÎåÄÎûµÏ†ÅÏù∏ Ï∂îÏ†ï
        print(f"\n‚ö†Ô∏è  WARNING: Hyperparameter BO is enabled!")
        print(f"   Each iteration may take ~{estimated_time_per_iter} times longer.")
        print(f"   Consider using smaller trial numbers for testing.")
        
        if args.mode == 'multiple' and args.num_runs > 5:
            print(f"   Multiple runs with BO may take very long time!")
            response = input("   Continue? (y/N): ")
            if response.lower() != 'y':
                print("   Aborted.")
                return
    
    try:
        if args.mode == 'single':
            # Îã®Ïùº Ïã§Ìñâ
            print(f"\nStarting single optimization run...")
            result = single_optimization_run(
                param_space=param_space,
                label_maps=label_maps,
                lookup=lookup,
                cost_budget=args.cost_budget,
                num_init_design=args.num_init_design,
                high_fidelity_ratio=args.high_fidelity_ratio,
                min_target=args.min_target,
                random_state=42,
                verbose=args.verbose,
                model_config=model_config,
                use_hyperparameter_bo=args.use_hyperparameter_bo,
                pretrain_bo_trials=args.pretrain_bo_trials,
                finetune_bo_trials=args.finetune_bo_trials,
                data_size=args.data_size
            )
            
            # Í≤∞Í≥º Ï∂úÎ†•
            print(f"\n=== Single Run Results ===")
            print(f"Total cost: {result['total_cost']:.2f}")
            print(f"Best value found: {result['best_so_far']:.4f}")
            print(f"Total iterations: {result['iterations']}")
            print(f"Target achieved: {'Yes' if result['best_so_far'] <= args.min_target else 'No'}")
            
            # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∏∞Î°ù Ï∂úÎ†•
            if args.use_hyperparameter_bo and result['hyperparameter_history']:
                print(f"\nüîß Hyperparameter Optimization Summary:")
                print(f"   Total BO iterations: {len(result['hyperparameter_history'])}")
                
                # ÎßàÏßÄÎßâ ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï∂úÎ†•
                last_hp = result['hyperparameter_history'][-1]
                if last_hp['pretrain_params']:
                    print(f"   Final pretrain params: {last_hp['pretrain_params']}")
                if last_hp['finetune_params']:
                    print(f"   Final finetune params: {last_hp['finetune_params']}")
            
            # ÏãúÍ∞ÅÌôî
            if args.plot_results:
                print("Generating plots...")
                plot_optimization_results([result], save_path='single_run_results.png')
        
        else:
            # Îã§Ï§ë Ïã§Ìñâ
            print(f"\nStarting {args.num_runs} optimization runs...")
            results = multiple_optimization_runs(
                param_space=param_space,
                label_maps=label_maps,
                lookup=lookup,
                num_runs=args.num_runs,
                cost_budget=args.cost_budget,
                num_init_design=args.num_init_design,
                high_fidelity_ratio=args.high_fidelity_ratio,
                min_target=args.min_target,
                model_config=model_config,
                save_results=args.save_results,
                results_filename=args.results_filename,
                use_hyperparameter_bo=args.use_hyperparameter_bo,
                pretrain_bo_trials=args.pretrain_bo_trials,
                finetune_bo_trials=args.finetune_bo_trials,
                data_size=args.data_size
            )
            
            # ÏãúÍ∞ÅÌôî
            if args.plot_results:
                print("Generating plots...")
                plot_optimization_results(results, save_path='multiple_runs_results.png')
    
    except KeyboardInterrupt:
        print("\n\nOptimization interrupted by user.")
    except Exception as e:
        print(f"\nError during optimization: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nOptimization completed!")


if __name__ == "__main__":
    main() 