{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/MultiFidelity-ProcessOpt/Perovskites/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from olympus.datasets import Dataset\n",
    "from olympus.objects import (\n",
    "\tParameterContinuous,\n",
    "\tParameterDiscrete, \n",
    "\tParameterCategorical, \n",
    "\tParameterVector\n",
    ")\n",
    "from olympus.campaigns import ParameterSpace, Campaign\n",
    "\n",
    "from atlas.planners.multi_fidelity.planner import MultiFidelityPlanner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "dataset = Dataset(kind='perovskites')\n",
    "NUM_RUNS = 2\n",
    "# BUDGET = 30\n",
    "COST_BUDGET = 50 # 200.\n",
    "NUM_INIT_DESIGN = 10\n",
    "NUM_CHEAP = 8\n",
    "\n",
    "# lookup table\n",
    "# organic --> cation --> anion --> bandgap_hse06/bandgap_gga\n",
    "LOOKUP = pickle.load(open('0.Data/lookup_table.pkl', 'rb'))\n",
    "# print(lookup.keys())\n",
    "# print(lookup['Ethylammonium']['Ge']['F'].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_options_descriptors(json_file):\n",
    "\twith open(json_file, 'r') as f:\n",
    "\t\tcontent = json.load(f)\n",
    "\n",
    "\toptions = list(content.keys())\n",
    "\tdescriptors = [list(content[opt].values()) for opt in options]\n",
    "\n",
    "\treturn options, descriptors\n",
    "\n",
    "def measure(params, s):\n",
    "\t# high-fidelity is hse06, low-fidelity is gga\n",
    "\tif s == 1.0:\n",
    "\t\tmeasurement = np.amin(\n",
    "\t\t\tLOOKUP[params.organic.capitalize()][params.cation][params.anion]['bandgap_hse06']\n",
    "\t\t)\n",
    "\telif s == 0.1:\n",
    "\t\tmeasurement = np.amin(\n",
    "\t\t\tLOOKUP[params.organic.capitalize()][params.cation][params.anion]['bandgap_gga']\n",
    "\t\t)\n",
    "\treturn measurement\n",
    "\n",
    "def get_min_hse06_bandgap(param_space):\n",
    "\torganic_options = [o.capitalize() for o in param_space[1].options]\n",
    "\tcation_options = [o.capitalize() for o in param_space[2].options]\n",
    "\tanion_options = [o.capitalize() for o in param_space[3].options]\n",
    "\n",
    "\thse06_bandgaps = []\n",
    "\tfor organic_option in organic_options:\n",
    "\t\tfor cation_option in cation_options:\n",
    "\t\t\tfor anion_option in anion_options:\n",
    "\t\t\t\thse06_bandgaps.append(\n",
    "\t\t\t\t\tnp.amin(\n",
    "\t\t\t\t\t\tLOOKUP[organic_option][cation_option][anion_option]['bandgap_hse06']\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\tmin_hse06_bandgap = np.amin(hse06_bandgaps)\n",
    "\treturn min_hse06_bandgap\n",
    "\n",
    "def compute_cost(params):\n",
    "\tcosts = params[:,0].astype(float)\n",
    "\treturn np.sum(costs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "\n",
    "class TransferLearningDNN:\n",
    "    def __init__(self, input_dim, hidden_dim=64, device='cpu'):\n",
    "        self.model = self._build_model(input_dim, hidden_dim).to(device)\n",
    "        self.input_dim = input_dim\n",
    "        self.device = device\n",
    "        self.scaler = None  # feature normalization (optional)\n",
    "\n",
    "    def _build_model(self, input_dim, hidden_dim):\n",
    "        # 심플한 2층 DNN\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def fit_scaler(self, X):\n",
    "        # min-max scaler\n",
    "        self.x_min = X.min(axis=0)\n",
    "        self.x_max = X.max(axis=0)\n",
    "\n",
    "    def pretrain(self, X_low, y_low, epochs=300, lr=1e-3, verbose=False):\n",
    "        # low-fidelity 데이터로 선학습\n",
    "        X_low, y_low = np.asarray(X_low), np.asarray(y_low).flatten()\n",
    "        if self.scaler is None:\n",
    "            self.fit_scaler(X_low)\n",
    "        X_low = self.transform(X_low)\n",
    "        X_tensor = torch.tensor(X_low, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y_low, dtype=torch.float32).to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.model(X_tensor).squeeze()\n",
    "            loss = loss_fn(pred, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and (epoch+1) % 50 == 0:\n",
    "                print(f'[Pretrain] Epoch {epoch+1}: Loss {loss.item():.4f}')\n",
    "\n",
    "    def finetune(self, X_high, y_high, epochs=100, lr=1e-4, verbose=False):\n",
    "        # high-fidelity 데이터로 파인튜닝\n",
    "        X_high, y_high = np.asarray(X_high), np.asarray(y_high).flatten()\n",
    "        X_tensor = torch.tensor(X_high, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y_high, dtype=torch.float32).to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.model(X_tensor).squeeze()\n",
    "            loss = loss_fn(pred, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and (epoch+1) % 20 == 0:\n",
    "                print(f'[Finetune] Epoch {epoch+1}: Loss {loss.item():.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        # (GA 등에서 호출) 입력 X에 대해 예측값 리턴\n",
    "        X = np.asarray(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(X_tensor).cpu().numpy().flatten()\n",
    "        return y_pred\n",
    "\n",
    "    def get_fitness_func(self, y_best=None, xi=0.01):\n",
    "        # pymoo 같은 GA에서 쓰기 위한 objective 함수 래퍼\n",
    "        # 여기선 EI(예상 개선량) 형태로 예시 구현\n",
    "        def fitness(x):\n",
    "            mu = self.predict(np.array([x]))[0]\n",
    "            if y_best is not None:\n",
    "                return -(y_best - mu - xi)  # maximize EI → minimize -EI\n",
    "            else:\n",
    "                return mu  # 단순 예측값 최적화\n",
    "        return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
